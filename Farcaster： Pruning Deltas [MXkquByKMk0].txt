So we previously established a model for hubs. Alice and Bob post messages, messages go to their hubs, each hub swaps messages with each other, and over time, the hubs have the exact same state of the world. That is, each hub is a full copy of all the messages from Alice and Bob in the network. But we haven't yet talked about who runs hubs. So it's important to figure out what the incentives are for operating these services. The first group of people incentivized to operate hubs are people who are building applications on the network. Now imagine FooCaster, that's actually a service that offers mobile and web apps and shows people advertisements and lets them use the network or maybe charges a subscription instead. FooCaster will want to run its own hub to make sure that it always has access to publish data to the network and to read data from the network. Now, not everybody starts out as serious as FooCaster. Some people just want to play around with the network, see what's possible on day one. So you can imagine that someone starts an API service like Cast API. Cast API runs its own hub, but it allows other applications to connect to this API and fetch data easily over HTTP, similar to what Infura or Alchemy do for Ethereum. This lets people start out by building simple applications using Cast API, and as they get more serious, they can graduate to running their own hub and actually build their own hub and add a third node to the network and so on. Now, there's one problem we haven't talked about yet, and that's what happens if Alice starts posting more and more messages to the network. Now, the immediate side effect is that hubs have to grow larger and larger to store all these messages over time. As hubs get larger, they get more expensive to operate. So you can imagine that there's a certain size at which FooCaster decides it's no longer viable for them to operate their own hub, and decides to just go over to Cast API instead. Alice is now forced to publish all her messages here, and FooCaster also has to read messages from here, and the network becomes less and less decentralized over time. Now, there's another problem that starts to emerge too. Imagine if Bob similarly started posting a lot of messages as well. But Bob's message, unlike Alice's, are all spammy. They're not valuable, they're just filled with junk characters, and Bob is just being annoying on the network. Now, Cast API might rightly build a tool that filters Bob's messages out of the network. So write something that says, hey, everything of Bob, we just will reject it from the hubs, we won't accept these messages. This starts to introduce another problem, which is that operators, now who are very few in number, can be selective about who they let on their network and who they don't let on their network. While they might start doing it for reasons of spam prevention or time this can be abused in certain ways, and it returns us to the old world of social media where a few powerful players can determine what happens on the network. That's not very ideal. One thing that's clear is it's important that there are limits to how big hubs can become. If hubs stay under a certain size, then they will be reasonably cost-effective to run, and then each application developer can run their own hub. But if they become too expensive or too tedious, it's very unlikely and we'll see a greater centralizing force on the world, and it's less likely that the network will remain decentralized over time. It's clear that we can't allow users to store an infinite amount of data on the hubs. There has to be some reasonable boundary on what users can do within a network at any given time. The first way the forecaster defines that is through a concept known as Delta pruning. The way we do that is we say that users are allowed to store messages and publish messages to the network, but messages older than a year get expired from the network. As Alice's message age, hubs will eventually drop them from the network when they become more than a year old. Now, these messages can be preserved on other archival systems. You can imagine an archive.org that stores old messages and all of these other systems that keep these messages. But the protocol itself doesn't guarantee the availability of these messages. It promises the last year's worth of messages, which in most cases is sufficient to build a valid functional social application. The last year's worth of data is probably 99 percent of the most important stuff that your users want to see, and that's actually sufficiently good. Now, some users may get very, very chatty and so in addition to this year's worth of data, there's also an upper size limit. That size limit, the details are in the protocol specification, but it's large enough so that the majority of users will never run into a size limit. But for a minority of users, again, if they exceed that limit, the older messages will get expired. So this pruning allows us to keep the network very robust and very decentralized by ensuring that hubs remain small, that each hub can be operated on a single AWS server, and that most developers will be able to run and operate a hub that has a full copy of the network and will be on an even playing field as far as building a social app goes. Now, there's a couple of interesting ideas that we will probably explore over time, not in the first release, but in future releases. For example, the idea that certain people can pay for additional storage. We know that a very small percentage of users on social media create a lot of content and maybe they want to preserve it for longer or have greater storage, and we might allow them to bid for larger storage slots so that they can store more of the data out of the network, and users who don't post as much will just not take up as much storage anyway, and so overall hubs can still remain small as the network scales. So there's a few other ideas on how to scale this, but overall bounding the size of each set and pruning them helps hubs stay decentralized as the forecaster network grows.